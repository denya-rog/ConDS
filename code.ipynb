{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tax_changing_successful\n",
      "changing_years_success\n",
      "droping_useless_success\n",
      "fillna_with_median_success\n",
      "drop ['id', 'fssdccount', 'fnsdcCount'] success\n",
      "features used for prediction: ['workerCount', 'pfrdcCount', 'hasCloudCryptCertificate', 'documentsCount', 'cnt_users', 'tax_sys', 'age_of_reg', 'age_of_adding']\n",
      "finding best classificator: ...\n",
      "knn checked ...\n",
      "logistic regression checked ...\n",
      "linear regression checked ...\n",
      "ridge cassifier checked ...\n",
      "decision tree checked ...\n",
      "random forest  checked ...\n",
      "best estimaor : RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=6.0, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=4,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Dec 26 21:13:06 2017\n",
    "\n",
    "@author: denya-rog\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def  tax_sys(train, li):\n",
    "    \"\"\"Change stings from pandas Serias to its numer in list of unique strings\"\"\"\n",
    "    for i in range(len (li)):\n",
    "        train[train == li[i]] = i\n",
    "    \n",
    "    return train\n",
    "\n",
    "            \n",
    "def how_old(data) :\n",
    "    \"\"\" Take pandas Series.\n",
    "    For making from continioudsly datafor making prediction more easy \n",
    "    and for avoiding overfitting\n",
    "    \n",
    "    Make numerical parametrs from date .\n",
    "            0- less then year\n",
    "            1- less then 2 years\n",
    "            2- less then 5 years\n",
    "            3- less then 10 years\n",
    "            4- more then 10 years \n",
    "            None- if there is no date or it has illegal format\"\"\"\n",
    "\n",
    "\n",
    "    out = data.fillna(\"01.01.0001\") \n",
    "    now = dt.datetime.now()\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        try:           \n",
    "            date = dt.datetime.strptime(str(out[i]), \"%Y-%m-%d\" )\n",
    "            \n",
    "        except ValueError:            \n",
    "            try:\n",
    "                date = dt.datetime.strptime(str(out[i]), \"%d.%m.%Y\" )\n",
    "                \n",
    "            except ValueError:\n",
    "                 date = dt.datetime(1,1,1)   \n",
    "            \n",
    "        difff = now - date\n",
    "        dif = difff.days//365\n",
    "        \n",
    "        if dif < 1:\n",
    "            out[i] = 0\n",
    "            \n",
    "        elif dif < 2 :\n",
    "            out[i] = 1\n",
    "            \n",
    "        elif dif < 5 :\n",
    "            out[i] = 2 \n",
    "            \n",
    "        elif dif < 10 :\n",
    "            out[i] = 3\n",
    "            \n",
    "        elif dif > 10 : \n",
    "            out[i] = 4\n",
    "            \n",
    "        else:\n",
    "            out[i] = None\n",
    "               \n",
    "    return out\n",
    "\n",
    "\n",
    "def estimate(norm_data,target):\n",
    "    \"\"\"Finding best predictor on train set. \n",
    "    Takes args: train set 2d-array like, target list-like.\n",
    "    Returns classifier with best bapams with best accuracy\"\"\"\n",
    "    \n",
    "    # making train /test split from train set for choosing best classificator\n",
    "    RAND_STATE = 42         #for some random\n",
    "    X_train,X_test,y_train,y_test = train_test_split(norm_data,target,test_size=0.4, random_state=RAND_STATE)\n",
    "    \n",
    "#    lists will contain best params and esimators with them\n",
    "    list_estim = []\n",
    "    list_best_param = []\n",
    "    \n",
    "#    making many classificators\n",
    "    print(\"finding best classificator: ...\")    \n",
    "    \n",
    "    #KNN\n",
    "    \n",
    "    neighbours={}\n",
    "    ALG={0:'ball_tree',1:'kd_tree'}\n",
    "    \n",
    "    for  i in range(10,40,5):\n",
    "        for j in range(2):\n",
    "            \n",
    "            neigh = KNeighborsClassifier(n_neighbors=3, leaf_size=i,algorithm =ALG[j])\n",
    "            neigh.fit(X_train, y_train)\n",
    "            neighbours[str(i)+\" alg=\"+str(j)] = neigh.score(X_test,y_test)\n",
    "            \n",
    "    list_best_param,list_estim = fil_best(neighbours, list_best_param,list_estim)        \n",
    "\n",
    "\n",
    "    print (\"knn checked ...\")\n",
    "    \n",
    "    #logistic Regression\n",
    "    \n",
    "    log_regr={}\n",
    "    \n",
    "    for c in  range(2,10,2):\n",
    "        for t  in [(0.1)**i for i in range(6)]:\n",
    "            \n",
    "            log = LogisticRegression(C=c, tol=t)\n",
    "            log.fit(X_train, y_train)\n",
    "            log_regr[str(c)+\" Tol=\"+str(t)] = log.score(X_test,y_test)  \n",
    "                   \n",
    "    list_best_param,list_estim = fil_best(log_regr, list_best_param,list_estim)\n",
    "\n",
    "    \n",
    "    print (\"logistic regression checked ...\")\n",
    "    \n",
    "    #linear Regression\n",
    "    \n",
    "    svc={}\n",
    "    \n",
    "    for c in  range(2,10,2):\n",
    "        for t  in [(0.1)**i for i in range(6)]:\n",
    "            \n",
    "            log = LinearSVC(C=c, tol=t)\n",
    "            log.fit(X_train, y_train)\n",
    "            svc[str(c)+\" Tol=\"+str(t)] = log.score(X_test,y_test)  \n",
    "                   \n",
    "    list_best_param,list_estim = fil_best(svc, list_best_param,list_estim)\n",
    "\n",
    "    \n",
    "    print (\"linear regression checked ...\")\n",
    "    \n",
    "    #Ridge Classifier\n",
    "    \n",
    "    ridge={}\n",
    "    \n",
    "    for a in  [(0.1)**i for i in range(-6,1,1)]:\n",
    "        for t  in [(0.1)**i for i in range(6)]:\n",
    "            \n",
    "            log = RidgeClassifier(alpha=a, tol=t)\n",
    "            log.fit(X_train, y_train)\n",
    "            ridge[str(a)+\" Tol=\"+str(t)] = log.score(X_test,y_test) \n",
    "                    \n",
    "    list_best_param,list_estim = fil_best(ridge, list_best_param,list_estim)\n",
    "\n",
    "    \n",
    "    print (\"ridge cassifier checked ...\")\n",
    "    \n",
    "    # Decision Tree\n",
    "    \n",
    "    tree={}\n",
    "    dept=[i for i in range(1,31,5)]\n",
    "    samp_spl=[i for i in range(2,11)]\n",
    "    \n",
    "    for a in  dept:\n",
    "        for t  in samp_spl:\n",
    "            \n",
    "            log = DecisionTreeClassifier(max_depth=a, min_samples_split=int(t))\n",
    "            log.fit(X_train, y_train)\n",
    "            tree[str(a)+\" Tol=\"+str(t)] = log.score(X_test,y_test)                         \n",
    "    \n",
    "    list_best_param,list_estim = fil_best(tree, list_best_param,list_estim)\n",
    "\n",
    "    \n",
    "    print (\"decision tree checked ...\")\n",
    "    \n",
    "    #Random Forest\n",
    "    \n",
    "    forest = {}\n",
    "    dept = [i for i in range(1,32,5)]\n",
    "    samp_spl = [i for i in range(2,11)]\n",
    "    \n",
    "    for a in  dept:\n",
    "        for t  in samp_spl:\n",
    "            \n",
    "            log = RandomForestClassifier(max_depth=a, min_samples_split=int(t))\n",
    "            log.fit(X_train, y_train)\n",
    "            forest[str(a)+\" Tol=\"+str(t)] = log.score(X_test,y_test)     \n",
    "                \n",
    "    list_best_param,list_estim = fil_best(forest, list_best_param,list_estim)\n",
    "\n",
    "    \n",
    "    print (\"random forest  checked ...\")\n",
    "    #chosing best estimatot, and fitting it\n",
    "    best_ind = list_best_param.index(max(list_best_param))\n",
    "    \n",
    "    return list_estim[best_ind]\n",
    "              \n",
    "   \n",
    "def fil_best(dic, list_best_param,list_estim):\n",
    "    \n",
    "    \"\"\"take dictionary with params of classifier, list with best parameters \n",
    "    and list with clasificators, returns list with new best parameters \n",
    "    and list with new clasificator \"\"\"\n",
    "    \n",
    "    best_key = [key for key,val in dic.items() if val == max(dic.values())]\n",
    "    best_value = [val for key,val in dic.items() if val == max(dic.values())]       \n",
    "    \n",
    "    list_best_param.append(best_value[0])    \n",
    "    A = float(re.findall(r'(^.*)\\s',best_key[0])[0])\n",
    "    B =  float(re.findall(r'[=](.*)$',best_key[0])[0]  )    \n",
    "    list_estim.append(RandomForestClassifier(max_depth=A, min_samples_split=int(B)))\n",
    "    \n",
    "    return list_best_param,list_estim\n",
    "        \n",
    "        \n",
    "def data_prediction(name1 , name2):\n",
    "    \n",
    "    \"\"\"main functon, take two names of files, preparing data,\n",
    "    choosing best predictor, creates new file with predicted data \"\"\"\n",
    "    \n",
    "    \n",
    "   \n",
    "    try:\n",
    "        test = pd.read_csv(name1, delimiter= \"\\t\", encoding= \"cp1251\")\n",
    "    except:\n",
    "         raise NameError('check  file or name of file for test')\n",
    "         \n",
    "    try:\n",
    "        train = pd.read_csv(name2, delimiter= \"\\t\",encoding= \"cp1251\")\n",
    "    except:\n",
    "         raise NameError('check  file or name of file for train')\n",
    " \n",
    "    print (\"load_success\")\n",
    "    \n",
    "    #making numeric tax system\n",
    "    \n",
    "        #creating list with unique tax systems\n",
    "    li1 = train[\"taxactionSystem\"].unique() \n",
    "    li2 = test[\"taxactionSystem\"].unique()\n",
    "    li = list(set(li1).union(li2))\n",
    "    \n",
    "    #creating numerical taxsystem feature     \n",
    "    \n",
    "    train[\"tax_sys\"] = tax_sys(train[\"taxactionSystem\"],li)\n",
    "    test[\"tax_sys\"] = tax_sys(test[\"taxactionSystem\"],li)\n",
    "    \n",
    "    print (\"tax_changing_successful\")\n",
    "    \n",
    "    # making new feature with numeric parametr with year of registerion and creation\n",
    "    \n",
    "    train[\"age_of_reg\"] = how_old(train[\"regdt\"])\n",
    "    test[\"age_of_reg\"] = how_old(test[\"regdt\"]) \n",
    "    \n",
    "    train[\"age_of_adding\"] = how_old(train[\"OrgCreationDate\"])\n",
    "    test[\"age_of_adding\"] = how_old(test[\"OrgCreationDate\"]) \n",
    "    \n",
    "    print(\"changing_years_success\")\n",
    "       \n",
    "     #dropong useless columns\n",
    "       \n",
    "    train = train.drop([\"regdt\",u\"OrgCreationDate\", \"taxactionSystem\"],axis = 1)\n",
    "    test = test.drop([\"regdt\",\"OrgCreationDate\", \"taxactionSystem\"],axis = 1)\n",
    "    \n",
    "    print(\"droping_useless_success\")\n",
    "    \n",
    "    #filling missing data with median\n",
    "    \n",
    "    for col in train:\n",
    "        train[col] = train[col].fillna(int(round(train[col].median()))).astype(int)\n",
    "    \n",
    "    for col in test:\n",
    "        test[col] = test[col].fillna(int(round(test[col].median()))).astype(int)\n",
    "\n",
    "    print(\"fillna_with_median_success\")\n",
    "   \n",
    "   #finding corelation among  features\n",
    "            \n",
    "    KOEF_CORR =0.7  #max value of koeficien of corelation\n",
    "    \n",
    "    cor = train.astype(float).corr().abs()\n",
    "    indices = np.where(cor > KOEF_CORR)   \n",
    "    indices = [(cor.index[x], cor.columns[y]) for x, y in zip(*indices) if x != y and x < y]\n",
    "    \n",
    " \n",
    "    # id containers\n",
    "    id_train = train[\"id\"]\n",
    "    id_test = test[\"id\"]\n",
    "    \n",
    "    drop_list=[\"id\"] \n",
    "        \n",
    "    for i in indices:\n",
    "        if cor[i[0]].sum()>cor[i[1]].sum() and i[0]not in drop_list:\n",
    "            drop_list.append(i[0])\n",
    "        else:\n",
    "            drop_list.append(i[1])\n",
    "            \n",
    "            \n",
    "    print (\"drop\",drop_list ,\"success\")\n",
    "    \n",
    "    train = train.drop(drop_list, axis=1)\n",
    "    test = test.drop(drop_list, axis=1)\n",
    "    \n",
    "    # data for training\n",
    "    norm_data = train.drop(\"is_prolong\", axis=1)\n",
    "    \n",
    "    print(\"features used for prediction:\", norm_data.columns.values.tolist())\n",
    "    \n",
    "    est = estimate(norm_data,train[\"is_prolong\"])\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"best estimaor :\",est)\n",
    "    \n",
    "    \n",
    "    est.fit(norm_data, train[\"is_prolong\"])\n",
    "    prediction = est.predict(test)\n",
    "    out = prediction\n",
    "    test[\"is_prolong\"] = out\n",
    "    test[\"id\"] = id_test\n",
    "\n",
    "\n",
    "    test.sort_values(by='id',inplace=True)\n",
    "    test.to_csv(\"test_predict.csv\", columns = [\"id\", \"is_prolong\"], encoding=\"utf-8\",index=False)\n",
    "\n",
    "\n",
    "\n",
    "data_prediction('test.csv', 'train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
